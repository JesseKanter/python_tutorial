{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-Introduction_Building_a_neural_network.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "UuuTEX1uxK-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction to Neural Networks:"
      ]
    },
    {
      "metadata": {
        "id": "LWc9ZHloax_2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 01 -- Writing the simplest neural network possible:\n",
        "\n",
        "This notebook explains how to build a very basic neural network in numpy. This network is trained to predict the output of a [XOR gate](https://en.wikipedia.org/wiki/XOR_gate)."
      ]
    },
    {
      "metadata": {
        "id": "nvLSvFnXa_sF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import the dependent library -- numpy:\n"
      ]
    },
    {
      "metadata": {
        "id": "QPY_MlEFbHlq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eNIxbmBXbPY8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function):\n",
        "\n",
        "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/rahulremanan/python_tutorial/master/Fundamentals_of_deep-learning/media/Logistic-curve.svg\">\n",
        "\n",
        "- The sigmoid function takes two input arguments: x and a boolean argument called 'derivative'.\n",
        "- When the boolean argument is set as true, the sigmoid function calculates the derivative of x.\n",
        "- The derivative of x is required when calculating error or performing back-propagation.\n",
        "- The sigmoid function runs in every single neuron.\n",
        "- The sigmoid funtion feeds forward the data by converting the numeric matrices to probablities.\n",
        "\n",
        "To implement the [logistic sigmoid function using numpy](https://stackoverflow.com/questions/3985619/how-to-calculate-a-logistic-sigmoid-function-in-python), we use the mathematical formula:\n",
        "\n",
        "![Sigmoid function formula](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/sigmoid_function_formula.png)\n",
        "\n",
        "### [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation):\n",
        "\n",
        "- Method to make the network better.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1NWiP4NAbOYY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x, derivative=False):\n",
        "  if derivative:\n",
        "    return (x*(1-x))\n",
        "  return (1/(1+np.exp(-x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W1TDFr6HlghF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "275beb03-7ce3-4f98-c88c-afb85bdd33ee",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527626094345,
          "user_tz": 240,
          "elapsed": 332,
          "user": {
            "displayName": "Rahul Remanan",
            "photoUrl": "//lh6.googleusercontent.com/-JV7tr1e65Qo/AAAAAAAAAAI/AAAAAAAAAAw/bjlFLKJeoJw/s50-c-k-no/photo.jpg",
            "userId": "117023126961214813918"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sigmoid(100, derivative=False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "4p1UNLTLfMpQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create an inut data matrix as numpy array:\n",
        "- Matrix with n number of dimensions."
      ]
    },
    {
      "metadata": {
        "id": "z7BWTaxmfXP8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x = np.asarray([[0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eunaVEqbmZIM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5f89eb18-c5a4-42ea-ad78-91ae0a616298",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527627424682,
          "user_tz": 240,
          "elapsed": 360,
          "user": {
            "displayName": "Rahul Remanan",
            "photoUrl": "//lh6.googleusercontent.com/-JV7tr1e65Qo/AAAAAAAAAAI/AAAAAAAAAAw/bjlFLKJeoJw/s50-c-k-no/photo.jpg",
            "userId": "117023126961214813918"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print (x.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r1mGhSApf-Ze",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the output data matrix as numpy array:"
      ]
    },
    {
      "metadata": {
        "id": "n6H3nAw5f-2n",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y = np.asarray([[0],\n",
        "                [1],\n",
        "                [1],\n",
        "                [0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYxrcX3uikpq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a random number seed:\n",
        "\n",
        "- Random number seeding is useful for producing reproducible results."
      ]
    },
    {
      "metadata": {
        "id": "Atv5s0UaikH_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "seed = 1\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mz5E6sOnjAFy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a synapse matrix:\n",
        "\n",
        "- A function applied to the syanpses.\n",
        "- For the first synapse, weights matrix of shape: input_shape_1 x input_shape_2 is created.\n",
        "- For the second synapse, weights matrix of shape: input_shape_2 x output_dim is created.\n",
        "-  This function also introduces the first hyper-parameter in neural network tuning called 'bias_val', which is the bias value for the synaptic function."
      ]
    },
    {
      "metadata": {
        "id": "_D5YYvZ_jAlU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "bias_val = 1\n",
        "\n",
        "output_dim = 1\n",
        "\n",
        "input_shape_1 = 3\n",
        "input_shape_2 = 4\n",
        "\n",
        "synapse_0 = 2*np.random.random((input_shape_1, input_shape_2)) - bias_val\n",
        "synapse_1 = 2*np.random.random((input_shape_2, output_dim)) - bias_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MgyoLCqPlIV5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the simple XOR gate neural network:\n",
        "\n",
        "- Note: There is no function that defines a neuron! In practice neuron is just an abstract concept to understand the probability function.\n",
        "- Continuously feeding the data throught the neural network.\n",
        "- Updating the weights of the network through backpropagation.\n",
        "- During the training the model becomes better and better in predicting the output values.\n",
        "- The layers are just a matrix multiplication functions that applies the sigmoid function to the synapse matrix and the corresponding layer.\n",
        "- Backpropagation portion of the training the machine learning portion of this code.\n",
        "- Backpropagation function reduces the prediction errors during each training step.\n",
        "- Synapses and weights are synonymous."
      ]
    },
    {
      "metadata": {
        "id": "cfNAijZ_lI4S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "training_steps = 60000\n",
        "update_freq = 6\n",
        "\n",
        "input_data = x\n",
        "output_data = y\n",
        "\n",
        "for i in range(training_steps):\n",
        "  # Creating the layers of the neural network:\n",
        "  layer_0 = input_data\n",
        "  layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
        "  layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
        "  \n",
        "  # Backpropagation:\n",
        "  output_error = output_data - layer_2\n",
        "  if ((i*update_freq) % training_steps ==0):\n",
        "    print ('Pprediction error during training :' + str(np.mean(np.abs(output_error))))\n",
        "    \n",
        "  # Layer-wsie delta function:\n",
        "  layer_2_delta = output_error*sigmoid(layer_2, derivative = True)  \n",
        "  layer_1_error = layer_2_delta.dot(synapse_1.T) # Matrix multiplication of the layer 2 delta with the transpose of the first synapse function.  \n",
        "  layer_1_delta = layer_1_error*sigmoid(layer_1, derivative = True)\n",
        "  \n",
        "  # Updating synapses or weights:\n",
        "  synapse_1 += layer_1.T.dot(layer_2_delta)\n",
        "  synapse_0 += layer_0.T.dot(layer_1_delta)\n",
        "\n",
        "print ('Training completed ...')\n",
        "print (layer_2)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T1NHH75GvL58",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 02 -- [Build a more complex neural network classifier using numpy](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/):"
      ]
    },
    {
      "metadata": {
        "id": "e4ZwQp-pyq_M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Importing dependent libraries:"
      ]
    },
    {
      "metadata": {
        "id": "rynRpfY5qjaX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt # pip3 install matplotlib\n",
        "import numpy as np # pip3 install numpy\n",
        "import sklearn # pip3 install scikit-learn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "import matplotlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rvrXHvqr0Gge",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Display plots inline and change default figure size:"
      ]
    },
    {
      "metadata": {
        "id": "VCunSkU5zdlc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DdKwQYtJ0aFM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate a dataset and create a plot:"
      ]
    },
    {
      "metadata": {
        "id": "B8lZ9n910Slo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "X, y = sklearn.datasets.make_moons(200, noise=0.20)\n",
        "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuc3DSeG4dcI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the logistic regression classifier:\n",
        "\n",
        "The classification problem can be summarized as creating a boundary between the red and the blue dots."
      ]
    },
    {
      "metadata": {
        "id": "Ez4hJ_9S33j6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "linear_classifier = sklearn.linear_model.LogisticRegressionCV()\n",
        "linear_classifier.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_gMrYdu5Ikg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the logistic regression classifier output:"
      ]
    },
    {
      "metadata": {
        "id": "SUPuKKEB5Acv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary(prediction_function):\n",
        "  # Setting minimum and maximum values for giving the plot function some padding\n",
        "  x_min, x_max = X[:, 0].min() - .5, \\\n",
        "                 X[:, 0].max() + .5\n",
        "  y_min, y_max = X[:, 1].min() - .5, \\\n",
        "                 X[:, 1].max() + .5\n",
        "  h = 0.01\n",
        "  # Generate a grid of points with distance h between them\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), \\\n",
        "                       np.arange(y_min, y_max, h))\n",
        "  # Predict the function value for the whole grid\n",
        "  Z = prediction_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  # Plotting the contour and training examples\n",
        "  plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.spectral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EgC6Vm5072hj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plotting the decision boundary:"
      ]
    },
    {
      "metadata": {
        "id": "pH_l5suh7w1f",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(lambda x: linear_classifier.predict(x))\n",
        "plt.title(\"Logistic Regression\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FzS_LhQYCXnW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a neural network:"
      ]
    },
    {
      "metadata": {
        "id": "xWz8z03d8LPg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_examples = len(X) # training set size\n",
        "nn_input_dim = 2 # input layer dimensionality\n",
        "nn_output_dim = 2 # output layer dimensionality"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hVpwJSH7Cub9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient descent parameters:"
      ]
    },
    {
      "metadata": {
        "id": "2s7Ap7_FCsub",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "epsilon = 0.01 # learning rate for gradient descent\n",
        "reg_lambda = 0.01 # regularization strength"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktuTAoe_C8xd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compute loss function on the dataset:\n",
        "\n",
        "Calculating predictions using forward propagation"
      ]
    },
    {
      "metadata": {
        "id": "j3PNCNomC65a",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def loss_function(model):\n",
        "  W1, b1, W2, b2 = model['W1'], \\\n",
        "                   model['b1'], \\\n",
        "                   model['W2'], \\\n",
        "                   model['b2']\n",
        "  z1 = X.dot(W1) + b1\n",
        "  a1 = np.tanh(z1)\n",
        "  z2 = a1.dot(W2) + b2\n",
        "  exp_scores = np.exp(z2)\n",
        "  probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "  # Calculating the loss function:\n",
        "  corect_logprobs = -np.log(probabilities[range(num_examples), y])\n",
        "  data_loss = np.sum(corect_logprobs)\n",
        "  # Adding the regulatization term to the loss function\n",
        "  data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
        "  return 1./num_examples * data_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u36QZJmtGZhu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function that predicts the output of either 0 or 1:"
      ]
    },
    {
      "metadata": {
        "id": "uOKPt6UIC5lm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def predict(model, x):\n",
        "    W1, b1, W2, b2 = model['W1'], \\\n",
        "                     model['b1'], \\\n",
        "                     model['W2'], \\\n",
        "                     model['b2']\n",
        "    # Design a network with forward propagation\n",
        "    z1 = x.dot(W1) + b1\n",
        "    a1 = np.tanh(z1)\n",
        "    z2 = a1.dot(W2) + b2\n",
        "    exp_scores = np.exp(z2)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    return np.argmax(probs, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6pzWRW7PHDRl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### This function learns parameters for the neural network and returns the model:\n",
        "- nn_hdim: Number of nodes in the hidden layer\n",
        "- num_passes: Number of passes through the training data for gradient descent\n",
        "- print_loss: If True, print the loss every 1000 iterations"
      ]
    },
    {
      "metadata": {
        "id": "kjZsvXVMG86T",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def build_model(nn_hdim, num_passes=20000, print_loss=False):\n",
        "    \n",
        "    # Initialize the parameters to random values. We need to learn these.\n",
        "    np.random.seed(0)\n",
        "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
        "    b1 = np.zeros((1, nn_hdim))\n",
        "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
        "    b2 = np.zeros((1, nn_output_dim))\n",
        "\n",
        "    # This is what we return at the end\n",
        "    model = {}\n",
        "    \n",
        "    # Gradient descent. For each batch...\n",
        "    for i in range(0, num_passes):\n",
        "\n",
        "        # Forward propagation\n",
        "        z1 = X.dot(W1) + b1\n",
        "        a1 = np.tanh(z1)\n",
        "        z2 = a1.dot(W2) + b2\n",
        "        exp_scores = np.exp(z2)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        # Backpropagation\n",
        "        delta3 = probs\n",
        "        delta3[range(num_examples), y] -= 1\n",
        "        dW2 = (a1.T).dot(delta3)\n",
        "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
        "        dW1 = np.dot(X.T, delta2)\n",
        "        db1 = np.sum(delta2, axis=0)\n",
        "\n",
        "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
        "        dW2 += reg_lambda * W2\n",
        "        dW1 += reg_lambda * W1\n",
        "\n",
        "        # Gradient descent parameter update\n",
        "        W1 += -epsilon * dW1\n",
        "        b1 += -epsilon * db1\n",
        "        W2 += -epsilon * dW2\n",
        "        b2 += -epsilon * db2\n",
        "        \n",
        "        # Assign new parameters to the model\n",
        "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
        "        \n",
        "        # Optionally print the loss.\n",
        "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
        "        if print_loss and i % 1000 == 0:\n",
        "          print(\"Loss after iteration %i: %f\" %(i, loss_function(model)))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0qsFUIRzHwFF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build a model with 50-dimensional hidden layer:"
      ]
    },
    {
      "metadata": {
        "id": "KsH3JKtVHni_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = build_model(50, print_loss=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "faX_dAZlPFLb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot the decision boundary:"
      ]
    },
    {
      "metadata": {
        "id": "0psfV-xdPAqi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(lambda x: predict(model, x))\n",
        "plt.title(\"Decision Boundary for hidden layer size  50\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "74-myoOdIQnE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing the hidden layers with varying sizes:"
      ]
    },
    {
      "metadata": {
        "id": "J-cqJVZTHytt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 32))\n",
        "hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]\n",
        "for i, nn_hdim in enumerate(hidden_layer_dimensions):\n",
        "    plt.subplot(5, 2, i+1)\n",
        "    plt.title('Hidden Layer size %d' % nn_hdim)\n",
        "    model = build_model(nn_hdim)\n",
        "    plot_decision_boundary(lambda x: predict(model, x))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MLCy6P16h8sa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a noisier, more complex dataset:"
      ]
    },
    {
      "metadata": {
        "id": "YBV41LHWIebT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "X, y = sklearn.datasets.make_moons(20000, noise=0.5)\n",
        "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WEn2RWkgiGOp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "linear_classifier = sklearn.linear_model.LogisticRegressionCV()\n",
        "linear_classifier.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3siSKYtLiPhg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(lambda x: linear_classifier.predict(x))\n",
        "plt.title(\"Logistic Regression\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itPMnFcLjhqb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_examples = len(X) # training set size\n",
        "nn_input_dim = 2 # input layer dimensionality\n",
        "nn_output_dim = 2 # output layer dimensionality"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YsC5K-7ugEKx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 03 -- Example illustrating the importance of[ learning rate](http://users.ics.aalto.fi/jhollmen/dippa/node22.html) in hyper-parameter tuning:\n",
        "\n",
        "- Learning rate is a decreasing function of time. \n",
        "- Two forms that are commonly used are:\n",
        "    * 1) a linear function of time \n",
        "    * 2) a function that is inversely proportional to the time t"
      ]
    },
    {
      "metadata": {
        "id": "0kG4EwVOjnTy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "epsilon = 0.01 # learning rate for gradient descent\n",
        "reg_lambda = 0.01 # regularization strength"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yb3E1Z-2iUOU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = build_model(50, print_loss=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SqDy-MxBhPv_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plotting the model that failed to learn, given a set of hyper-parameters:"
      ]
    },
    {
      "metadata": {
        "id": "jI6ZFYrmg66w",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(lambda x: predict(model, x))\n",
        "plt.title(\"Decision Boundary for hidden layer size  50\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xakIXy_qhZzc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Adjusting the learning rate such that the neural network re-starts learning:"
      ]
    },
    {
      "metadata": {
        "id": "SdxhhV16i4R_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "epsilon = 1e-6 # learning rate for gradient descent\n",
        "reg_lambda = 0.01 # regularization strength"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NEvPH7E8l-lI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = build_model(50, print_loss=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r98-w9LWhjj0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plotting the decision boundary layer generated by an improved neural network model:"
      ]
    },
    {
      "metadata": {
        "id": "YfkTEZaFnlwY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(lambda x: predict(model, x))\n",
        "plt.title(\"Decision Boundary for hidden layer size  50\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7UYDOuTwScqU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 04 -- Building a neural network using [tensorflow](https://www.tensorflow.org/):\n",
        "\n",
        "- A neural network that predicts the y value given an x value.\n",
        "- Implemented using tensorflow, an open-source deep-learning library."
      ]
    },
    {
      "metadata": {
        "id": "5zHIZav4TOyI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import dependent libraries:"
      ]
    },
    {
      "metadata": {
        "id": "2pejP8ZqSjCa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGmGnCPRTSap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a synthetic dataset for training and generating predictions:"
      ]
    },
    {
      "metadata": {
        "id": "HI1b0M0UTMe8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x_data = np.float32(np.random.rand(2,500))\n",
        "y_data = np.dot([0.5, 0.7], x_data) + 0.6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PyP8XW34UCUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Variable objects store tensors in tensorflow.\n",
        "- Tensorflow considers all input data tensors.\n",
        "- Tensors are 3 dimensional matrices."
      ]
    },
    {
      "metadata": {
        "id": "IY2GHcRVVX24",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Constructing a linear model:"
      ]
    },
    {
      "metadata": {
        "id": "IaTUubcYT32h",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "bias = tf.Variable(tf.zeros([1]))\n",
        "synapses = tf.Variable(tf.random_uniform([1, 2], -1, 1)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ySh6emwOVHT7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y = tf.matmul(synapses, x_data) + bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xyRSzl0YVgsV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient descent optimizer:\n",
        "\n",
        "- Imagine the valley with a ball.\n",
        "- The goal of the optimizer is to localize the ball to the lowest point in the valley.\n",
        "- Loss function will be reduced over the training.\n",
        "- Mean squared error as the loss function."
      ]
    },
    {
      "metadata": {
        "id": "e2nZ8XKTVxhk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(y - y_data))\n",
        "optimizer = tf.train.GradientDescentOptimizer(lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NDrNdKguXAyU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training function:\n",
        "\n",
        "- In tensorflow the computation is wrapped inside a graph.\n",
        "- Tensorflow makes it easier to visualize the training sessions."
      ]
    },
    {
      "metadata": {
        "id": "u5a7Sdk6W_ka",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zPulbwY2XfxO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialize the variables for the computational graph:"
      ]
    },
    {
      "metadata": {
        "id": "WqGJ2gp2Xlef",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ODVgyp6SXuJJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Launching the tensorflow computational graph:"
      ]
    },
    {
      "metadata": {
        "id": "Zlhj-p1tXynA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ocLrCdFX7wp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the model:"
      ]
    },
    {
      "metadata": {
        "id": "9RkVvUDIX9qt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "training_steps = 60000\n",
        "\n",
        "for step in range (0, training_steps):\n",
        "  sess.run(train)\n",
        "  if step % 1000 == 0:\n",
        "    print ('Current training session: ' + str(step) + str(sess.run(synapses))+ str(sess.run(bias)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}