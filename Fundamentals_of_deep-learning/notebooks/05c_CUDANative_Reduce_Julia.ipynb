{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia -- CUDA Native programming\n",
    "## Author: Dr. Rahul Remanan\n",
    "\n",
    "* A test notebook for CUDAnative package in Julia.\n",
    "* Based on the [Julia CUDAnative reduce example](https://github.com/JuliaGPU/CUDAnative.jl/tree/master/examples/reduce).\n",
    "* CUDA is Nvidia's general purpose compute using GPU library.\n",
    "* Using CUDAnative, high-performance GPU code using CUDA kernels can be written in Julia.\n",
    "* CUDAnative sits at the same abstraction level as CUDA C.\n",
    "* Highly integrated with Julia compiler and LLVM framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.1.0-DEV.255\n",
      "Commit 26b6a5811c (2018-09-13 21:44 UTC)\n",
      "Platform Info:\n",
      "  OS: Linux (x86_64-linux-gnu)\n",
      "  CPU: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-6.0.1 (ORCJIT, broadwell)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Vector addition using GPU -- Using CUDAdrv.jl](https://julialang.org/blog/2017/03/cudanative)\n",
    "\n",
    "* Small demo for GPU programming capabilities in Julia.\n",
    "* Uses functionality from CUDAdrv.jl\n",
    "* User friendly wrapper for interacting with CUDA hardware\n",
    "* Provides and array type: CuArray\n",
    "* Performs memory management through garbage collection integration with Julia\n",
    "* @elapsed using GPU events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using CUDAdrv, CUDAnative\n",
    "using Base.Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a vector addition function using CUDA kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function kernel_vadd(a, b, c)\n",
    "    # from CUDAnative: (implicit) CuDeviceArray type,\n",
    "    #                  and thread/block intrinsics\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    c[i] = a[i] + b[i]\n",
    "\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify target GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev = CuDevice(0)\n",
    "ctx = CuContext(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len = 512\n",
    "a = rand(Int, len)\n",
    "b = rand(Int, len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate resources and upload variables on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_a = CuArray(a)\n",
    "d_b = CuArray(b)\n",
    "d_c = similar(d_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the code on the GPU and fetch the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@cuda (1,len) kernel_vadd(d_a, d_b, d_c)    # from CUDAnative.jl\n",
    "c = Array(d_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results from the code executed in CUDA kernels with CPU execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@test c == a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free-up the GPU resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "destroy(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Parallel reduction using CUDA kernel -- Using CUDAnative.jl](https://julialang.org/blog/2017/03/cudanative)\n",
    "\n",
    "* CUDAnative.jl takes care of all things related to native GPU programming\n",
    "\n",
    "    1) interfacing with Julia: repurpose the compiler to emit GPU-compatible LLVM IR (no calls to CPU libraries, simplified exceptions, …)\n",
    "    \n",
    "    2) interfacing with LLVM (using LLVM.jl): optimize the IR, and compile to PTX\n",
    "    \n",
    "    3) interfacing with CUDA (using CUDAdrv.jl): compile PTX to SASS, and upload it to the GPU\n",
    "    \n",
    "    \n",
    "* These functionalities hidden behind the call to @cuda\n",
    "\n",
    "* Intrinsics: special functions and macros that provide functionality hard or impossible to express using normal functions. For example, the {thread,block,grid}{Idx,Dim} functions provide access to the size and index of each level of work.\n",
    "\n",
    "* Creating local shared memory: @cuStaticSharedMem and @cuDynamicSharedMem macros\n",
    "\n",
    "* Display formatted string from within a kernel function: @cuprintf\n",
    "\n",
    "* [Math functions similar to the Julia standard library](https://github.com/JuliaGPU/CUDAnative.jl/blob/0721783db9ac4cc2c2948cbf8cbff4aa5f7c4271/src/device/intrinsics.jl#L499-L807)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Test\n",
    "using CUDAdrv, CUDAnative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduce_warp (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Main implementation\n",
    "#\n",
    "\n",
    "# Reduce a value across a warp\n",
    "@inline function reduce_warp(op::F, val::T)::T where {F<:Function,T}\n",
    "    offset = CUDAnative.warpsize() ÷ 2\n",
    "    # TODO: this can be unrolled if warpsize is known...\n",
    "    while offset > 0\n",
    "        val = op(val, shfl_down(val, offset))\n",
    "        offset ÷= 2\n",
    "    end\n",
    "    return val\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduce_block (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce a value across a block, using shared memory for communication\n",
    "@inline function reduce_block(op::F, val::T)::T where {F<:Function,T}\n",
    "    # shared mem for 32 partial sums\n",
    "    shared = @cuStaticSharedMem(T, 32)\n",
    "\n",
    "    wid, lane = fldmod1(threadIdx().x, CUDAnative.warpsize())\n",
    "\n",
    "    # each warp performs partial reduction\n",
    "    val = reduce_warp(op, val)\n",
    "\n",
    "    # write reduced value to shared memory\n",
    "    if lane == 1\n",
    "        @inbounds shared[wid] = val\n",
    "    end\n",
    "\n",
    "    # wait for all partial reductions\n",
    "    sync_threads()\n",
    "\n",
    "    # read from shared memory only if that warp existed\n",
    "    @inbounds val = (threadIdx().x <= fld(blockDim().x, CUDAnative.warpsize())) ? shared[lane] : zero(T)\n",
    "\n",
    "    # final reduce within first warp\n",
    "    if wid == 1\n",
    "        val = reduce_warp(op, val)\n",
    "    end\n",
    "\n",
    "    return val\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduce_grid (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce an array across a complete grid\n",
    "function reduce_grid(op::F, input::CuDeviceVector{T}, output::CuDeviceVector{T},\n",
    "                     len::Integer) where {F<:Function,T}\n",
    "\n",
    "    # TODO: neutral element depends on the operator (see Base's 2 and 3 argument `reduce`)\n",
    "    val = zero(T)\n",
    "\n",
    "    # reduce multiple elements per thread (grid-stride loop)\n",
    "    # TODO: step range (see JuliaGPU/CUDAnative.jl#12)\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    step = blockDim().x * gridDim().x\n",
    "    while i <= len\n",
    "        @inbounds val = op(val, input[i])\n",
    "        i += step\n",
    "    end\n",
    "\n",
    "    val = reduce_block(op, val)\n",
    "\n",
    "    if threadIdx().x == 1\n",
    "        @inbounds output[blockIdx().x] = val\n",
    "    end\n",
    "\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu_reduce (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reduce a large array.\n",
    "Kepler-specific implementation, ie. you need sm_30 or higher to run this code.\n",
    "\"\"\"\n",
    "function gpu_reduce(op::Function, input::CuVector{T}, output::CuVector{T}) where {T}\n",
    "    len = length(input)\n",
    "\n",
    "    # TODO: these values are hardware-dependent, with recent GPUs supporting more threads\n",
    "    threads = 512\n",
    "    blocks = min((len + threads - 1) ÷ threads, 1024)\n",
    "\n",
    "    # the output array must have a size equal to or larger than the number of thread blocks\n",
    "    # in the grid because each block writes to a unique location within the array.\n",
    "    if length(output) < blocks\n",
    "        throw(ArgumentError(\"output array too small, should be at least $blocks elements\"))\n",
    "    end\n",
    "\n",
    "    @cuda blocks=blocks threads=threads reduce_grid(op, input, output, len)\n",
    "    @cuda threads=1024 reduce_grid(op, output, output, blocks)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if capability(device()) < v\"3.0\"\n",
    "    @warn(\"this example requires a newer GPU\")\n",
    "    exit(0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element Array{Int32,1}:\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len = 10^7\n",
    "input = ones(Int32, len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU Reduce example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_val = reduce(+, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDAnative Reduce example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_input = CuArray(input)\n",
    "gpu_output = similar(gpu_input)\n",
    "gpu_reduce(+, gpu_input, gpu_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array(gpu_output)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    gpu_input = CuArray(input)\n",
    "    gpu_output = similar(gpu_input)\n",
    "    gpu_reduce(+, gpu_input, gpu_output)\n",
    "    gpu_val = Array(gpu_output)[1]\n",
    "    @assert cpu_val == gpu_val\n",
    "    Test.@test cpu_val == gpu_val\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
