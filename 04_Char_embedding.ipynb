{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Mon Jul  3 00:15:19 2017\n",
    "\n",
    "@author: rahul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Embedding\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "embeddings_path = \"./glove.840B.300d-char.txt\" # http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "embedding_dim = 300\n",
    "batch_size = 128\n",
    "use_pca = False\n",
    "lr = 0.001\n",
    "lr_decay = 1e-4\n",
    "maxlen = 300\n",
    "consume_less = 2   # 0 for cpu, 2 for gpu\n",
    "\n",
    "text = open('./Alice.txt').read()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen), dtype=np.int)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t] = char_indices[char]\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# test code to sample on 10% for functional model testing\n",
    "\n",
    "def random_subset(X, y, p=0.1):\n",
    "\n",
    "    idx = np.random.randint(X.shape[0], size=int(X.shape[0] * p))\n",
    "    X = X[idx, :]\n",
    "    y = y[idx]\n",
    "    return (X, y)\n",
    "\n",
    "\n",
    "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "def generate_embedding_matrix(embeddings_path):\n",
    "    print('Processing pretrained character embeds...')\n",
    "    embedding_vectors = {}\n",
    "    with open(embeddings_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.strip().split(\" \")\n",
    "            vec = np.array(line_split[1:], dtype=float)\n",
    "            char = line_split[0]\n",
    "            embedding_vectors[char] = vec\n",
    "\n",
    "    embedding_matrix = np.zeros((len(chars), 300))\n",
    "    #embedding_matrix = np.random.uniform(-1, 1, (len(chars), 300))\n",
    "    for char, i in char_indices.items():\n",
    "        #print (\"{}, {}\".format(char, i))\n",
    "        embedding_vector = embedding_vectors.get(char)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # Use PCA from sklearn to reduce 300D -> 50D\n",
    "    if use_pca:\n",
    "        pca = PCA(n_components=embedding_dim)\n",
    "        pca.fit(embedding_matrix)\n",
    "        embedding_matrix_pca = np.array(pca.transform(embedding_matrix))\n",
    "        embedding_matrix_result = embedding_matrix_pca\n",
    "        print (embedding_matrix_pca)\n",
    "        print (embedding_matrix_pca.shape)\n",
    "    else:\n",
    "        embedding_matrix_result = embedding_matrix\n",
    "    return embedding_matrix_result\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "main_input = Input(shape=(maxlen,))\n",
    "embedding_matrix = generate_embedding_matrix(embeddings_path)\n",
    "embedding_layer = Embedding(\n",
    "len(chars), embedding_dim, input_length=maxlen,\n",
    "weights=[embedding_matrix])\n",
    "# embedding_layer = Embedding(\n",
    "#     len(chars), embedding_dim, input_length=maxlen)\n",
    "embedded = embedding_layer(main_input)\n",
    "\n",
    "    # RNN Layer\n",
    "rnn = LSTM(256, implementation=consume_less)(embedded)\n",
    "\n",
    "aux_output = Dense(len(chars))(rnn)\n",
    "aux_output = Activation('softmax', name='aux_out')(aux_output)\n",
    "\n",
    "    # Hidden Layers\n",
    "hidden_1 = Dense(512, use_bias=False)(rnn)\n",
    "hidden_1 = BatchNormalization()(hidden_1)\n",
    "hidden_1 = Activation('relu')(hidden_1)\n",
    "\n",
    "hidden_2 = Dense(256, use_bias=False)(hidden_1)\n",
    "hidden_2 = BatchNormalization()(hidden_2)\n",
    "hidden_2 = Activation('relu')(hidden_2)\n",
    "\n",
    "main_output = Dense(len(chars))(hidden_2)\n",
    "main_output = Activation('softmax', name='main_out')(main_output)\n",
    "\n",
    "model = Model(inputs=main_input, outputs=[main_output, aux_output])\n",
    "\n",
    "optimizer = Adam(lr=lr, decay=lr_decay)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=optimizer, loss_weights=[1., 0.2])\n",
    "model.summary()\n",
    "\n",
    "# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-6) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "if not os.path.exists('./output'):\n",
    "    os.makedirs('./output')\n",
    "\n",
    "f = open('./log.csv', 'w')\n",
    "log_writer = csv.writer(f)\n",
    "log_writer.writerow(['iteration', 'batch', 'batch_loss',\n",
    "                     'epoch_loss', 'elapsed_time'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    \"./output/model.hdf5\", monitor='main_out_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "class BatchLossLogger(Callback):\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('main_out_loss'))\n",
    "        if batch % 50 == 0:\n",
    "            log_writer.writerow([iteration, batch,\n",
    "                                 logs.get('main_out_loss'),\n",
    "                                 np.mean(self.losses),\n",
    "                                 round(time.time() - start_time, 2)])\n",
    "\n",
    "start_time = time.time()\n",
    "for iteration in range(1, 20):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "\n",
    "    logger = BatchLossLogger()\n",
    "    # X_train, y_train = random_subset(X, y)\n",
    "    # history = model.fit(X_train, [y_train, y_train], batch_size=batch_size,\n",
    "    #                     epochs=1, callbacks=[logger, checkpointer])\n",
    "    history = model.fit(X, [y, y], batch_size=batch_size,\n",
    "                        epochs=10, callbacks=[logger, checkpointer])\n",
    "    loss = str(history.history['main_out_loss'][-1]).replace(\".\", \"_\")\n",
    "\n",
    "    f2 = open('./output/iter-{:02}-{:.6}.txt'.format(iteration, loss), 'w')\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "        f2.write('----- diversity:' + ' ' + str(diversity) + '\\n')\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        f2.write('----- Generating with seed: \"' + sentence + '\"' + '\\n---\\n')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(1200):\n",
    "            x = np.zeros((1, maxlen), dtype=np.int)\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t] = char_indices[char]\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0][0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        f2.write(generated + '\\n')\n",
    "        print()\n",
    "    f2.close()\n",
    "\n",
    "    # Write embeddings for current characters to file\n",
    "    # The second layer has the embeddings.\n",
    "\n",
    "    embedding_weights = model.layers[1].get_weights()[0]\n",
    "    f3 = open('./output/char-embeddings.txt', 'w')\n",
    "    for char in char_indices:\n",
    "        if ord(char) < 128:\n",
    "            embed_vector = embedding_weights[char_indices[char], :]\n",
    "            f3.write(char + \" \" + \" \".join(str(x)\n",
    "                                           for x in embed_vector) + \"\\n\")\n",
    "    f3.close()\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
